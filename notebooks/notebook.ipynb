{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24f7fc1a08b42468d8d3f56bcf5992e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ansel/ai/podcast-pipeline/.venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import re\n",
    "import sqlite3\n",
    "\n",
    "dbFile = \"podcasts.db\"\n",
    "\n",
    "from gliner import GLiNER\n",
    "\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_medium-v2.1\")\n",
    "from ollama import Client\n",
    "import tiktoken\n",
    "from chonkie import SDPMChunker\n",
    "\n",
    "parent_dir = str(Path().resolve().parents[0])\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "from app.utils import ShowMetadataList, ShowMetadata, metadata_to_dict, sanitize\n",
    "\n",
    "data = ShowMetadataList(\n",
    "    shows=[\n",
    "        ShowMetadata(\n",
    "            \"rotl\",\n",
    "            Path(\"../files/meta/rotl_dates.txt\"),\n",
    "            Path(\"../files/meta/rotl_titles.txt\"),\n",
    "        ),\n",
    "        ShowMetadata(\n",
    "            \"roadwork\",\n",
    "            Path(\"../files/meta/roadwork_dates.txt\"),\n",
    "            Path(\"../files/meta/roadwork_titles.txt\"),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dates_titles = metadata_to_dict(data)\n",
    "\n",
    "\n",
    "def get_meta(file):\n",
    "    file_name = file.name\n",
    "    episode_number = file_name.split(\"_-_\")[0]\n",
    "    episode_date = dates_titles[\"rotl\"][\"dates\"][episode_number]\n",
    "    episode_title = dates_titles[\"rotl\"][\"titles\"][episode_number]\n",
    "    return (file_name, episode_number, episode_date, episode_title)\n",
    "\n",
    "\n",
    "def get_entities(transcript):\n",
    "    names = set()\n",
    "    for line in transcript.splitlines():\n",
    "        speaker, text = line.split(\": \")\n",
    "        entities = model.predict_entities(text, [\"Person\"], threshold=0.5)\n",
    "        for entity in entities:\n",
    "            names.add(entity[\"text\"])\n",
    "    return sorted(list(names))\n",
    "\n",
    "\n",
    "client = Client(host=\"https://mlkyway.anselbrandt.net/ollama\")\n",
    "\n",
    "\n",
    "def ask_llm(context, model=\"gemma2:27b\"):\n",
    "    response = client.chat(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": context,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    response_content = response[\"message\"][\"content\"]\n",
    "    return response_content\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    chunks = text.split(\"\\n\\n\")\n",
    "    response = chunks[1]\n",
    "    return re.sub(r\"\\*\", \"\", response)\n",
    "\n",
    "\n",
    "chunker = SDPMChunker(\n",
    "    embedding_model=\"minishlab/potion-base-8M\",\n",
    "    threshold=0.5,\n",
    "    chunk_size=512,\n",
    "    min_sentences=1,\n",
    "    skip_window=1,\n",
    "    delim=\"\\n\",\n",
    ")\n",
    "\n",
    "\n",
    "def num_tokens(string: str) -> int:\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "def clean_name_response(text):\n",
    "    if \"\\n\" in text:\n",
    "        lines = re.sub(r\"\\n+\", \"\\n\", text).splitlines()\n",
    "        clean_lines = [line for line in lines if \"Let me know if\" not in line]\n",
    "        return \"\\n\".join(clean_lines)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "\n",
    "def clean_transcript(file):\n",
    "    lines = open(file).read().split(\"\\n\\n\")\n",
    "    return \"\\n\".join([line for line in lines if len(line.split(\": \")) == 2])\n",
    "\n",
    "\n",
    "def get_names(transcript):\n",
    "    named_entities = get_entities(transcript)\n",
    "    entities_query = \"Which of these are people's names? Only return results if they are people's names.\"\n",
    "    entities_context = f\"{entities_query}\\n\\n{named_entities}\"\n",
    "    entities_response = ask_llm(entities_context)\n",
    "    names = clean(entities_response)\n",
    "    return [name.strip() for name in names.splitlines()]\n",
    "\n",
    "\n",
    "def create_chunks_db():\n",
    "    conn = sqlite3.connect(dbFile)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\n",
    "        \"\"\"CREATE TABLE IF NOT EXISTS chunks (\n",
    "        id integer primary key,\n",
    "        filename text,\n",
    "        showname text,\n",
    "        episode text,\n",
    "        title text,\n",
    "        date text,\n",
    "        idx integer,\n",
    "        chunk text\n",
    "        )\"\"\"\n",
    "    )\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def create_names_db():\n",
    "    conn = sqlite3.connect(dbFile)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\n",
    "        \"\"\"CREATE TABLE IF NOT EXISTS names (\n",
    "        id integer primary key,\n",
    "        filename text,\n",
    "        showname text,\n",
    "        episode text,\n",
    "        title text,\n",
    "        date text,\n",
    "        name integer,\n",
    "        text text\n",
    "        )\"\"\"\n",
    "    )\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def insert_chunks(\n",
    "    chunks, file_name, show_name, episode_number, episode_title, episode_date\n",
    "):\n",
    "    conn = sqlite3.connect(dbFile)\n",
    "    c = conn.cursor()\n",
    "    filename = file_name\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        c.execute(\n",
    "            \"INSERT INTO chunks VALUES (NULL, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "            (\n",
    "                filename,\n",
    "                show_name,\n",
    "                episode_number,\n",
    "                episode_title,\n",
    "                episode_date,\n",
    "                idx,\n",
    "                chunk,\n",
    "            ),\n",
    "        )\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def insert_name(\n",
    "    name, text, file_name, show_name, episode_number, episode_title, episode_date\n",
    "):\n",
    "    conn = sqlite3.connect(dbFile)\n",
    "    c = conn.cursor()\n",
    "    filename = file_name\n",
    "    c.execute(\n",
    "        \"INSERT INTO names VALUES (NULL, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "        (\n",
    "            filename,\n",
    "            show_name,\n",
    "            episode_number,\n",
    "            episode_title,\n",
    "            episode_date,\n",
    "            name,\n",
    "            text,\n",
    "        ),\n",
    "    )\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = Path(\"../files/bulk/rotl\")\n",
    "files = [file for file in dir.iterdir() if \".txt\" in file.name]\n",
    "\n",
    "names_dir = Path() / \"names\"\n",
    "names_dir.mkdir(exist_ok=True)\n",
    "\n",
    "results = {}\n",
    "for file in sorted(files):\n",
    "    file_name, episode_number, episode_date, episode_title = get_meta(file)\n",
    "    transcript = clean_transcript(file)\n",
    "    chunks = [chunk.text for chunk in chunker(transcript)]\n",
    "    for chunk in chunks:\n",
    "        for name in [\"Ariella\", \"Marlo\", \"Eleanor\", \"Madeline\"]:\n",
    "            if name in chunk:\n",
    "                if name in results:\n",
    "                    results[name].append(chunk)\n",
    "                else:\n",
    "                    results[name] = [chunk]\n",
    "\n",
    "for name, chunks in results.items():\n",
    "    out_path = names_dir / f\"{sanitize(name)}.txt\"\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(f\"{\"\\n\".join(chunks)}\\n\\nWho is {name}?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_files = [file for file in names_dir.iterdir()]\n",
    "results_dir = Path() / \"results\"\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for file in names_files:\n",
    "    context = open(file, \"r\").read()\n",
    "    results = ask_llm(context)\n",
    "    out_path = results_dir / file.name\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11de0eb2c1f448f0ab900fb2147e4c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gliner import GLiNER\n",
    "\n",
    "model = GLiNER.from_pretrained(\"knowledgator/gliner-multitask-large-v0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [file for file in Path(\"../files/bulk/rotl\").iterdir() if \".txt\" in file.name]\n",
    "\n",
    "transcripts = []\n",
    "\n",
    "for file in files:\n",
    "    transcript = open(file, \"r\").read().split(\"\\n\\n\")\n",
    "    text = \"\\n\".join(transcript)\n",
    "    transcripts.append(text)\n",
    "\n",
    "with open(\"Roderick_on_the_Line.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(transcripts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
